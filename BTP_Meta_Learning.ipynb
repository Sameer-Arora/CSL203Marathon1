{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BTP_Meta_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-Arora/CSL203Marathon1/blob/master/BTP_Meta_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iH2vvd0Ly0q",
        "colab_type": "code",
        "outputId": "37b0459e-2c17-441d-c816-5e6a1f0b5a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        " \n",
        "# import os  \n",
        "# os.chdir(\"/content/drive/My Drive/BTP/Codes/learning-to-learn-by-pytorch-master/\")\n",
        "\n",
        "import argparse  \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Softmax,ReLU,Conv2D,Activation,MaxPool2D,LSTMCell,RNN\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "LOGDIR = \"sample_data/\"\n",
        "os.chdir(LOGDIR)\n",
        "print(os.getcwd())\n",
        "\n",
        "train_summary_writer = tf.summary.create_file_writer(LOGDIR)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "/content/sample_data/sample_data/sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dfQpM43MTkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_meta_model(n_hidden,n_layers,n_trainable_vars):\n",
        "    cell_list = []\n",
        "    lstm_cell1 = LSTMCell(n_hidden)\n",
        "    lstm_cell2 = LSTMCell(n_hidden)\n",
        "    lstm_cell3 = LSTMCell(1)\n",
        "    for i in range(n_trainable_vars):\n",
        "        cell_list.append( tf.keras.layers.StackedRNNCells([lstm_cell1]+[lstm_cell2 for _ in range(n_layers-2)] +[lstm_cell3] ) ) # n_layers = 2 according to the paper.\n",
        "#         cell_list.append( tf.keras.layers.StackedRNNCells([lstm_cell2 for _ in range(n_layers)] ) ) # n_layers = 2 according to the paper.\n",
        "    return cell_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-pKXh5dM4-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(out_dim):\n",
        "  model= Sequential()\n",
        "  model.add(Dense(out_dim+2,dynamic=True))\n",
        "  model.add(Dense(out_dim,dynamic=True))\n",
        "  return model  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nuki0nZC8RE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_parameters(vars):\n",
        "  total_vars=0\n",
        "  for count,var in enumerate(vars):\n",
        "#     print(grad_var)\n",
        "    total_vars += tf.reshape(var,[-1]).shape[0]\n",
        "    print(total_vars)\n",
        "  \n",
        "  return total_vars\n",
        "#   for grad,var in grads_vars:\n",
        "    \n",
        "def reshape_params(grads_vars_):\n",
        "  print(grads_vars_)\n",
        "#   print(grads_vars_[0][0].shape)\n",
        "#   print(grads_vars_[0][1].shape)\n",
        "  grads_vars=[[],[]]\n",
        "  \n",
        "  for count,grad_var in enumerate(grads_vars_):\n",
        "#   for grad_var in grads_vars_:\n",
        "#     print(grad_var)\n",
        "    grad=grad_var[0]\n",
        "    var= grad_var[1]\n",
        "#     print(grad.shape)\n",
        "#     print(var.shape)\n",
        "    grads_vars[0]+= tf.unstack(tf.reshape(grad,[-1]))\n",
        "    grads_vars[1]+= tf.unstack( tf.reshape(var,[-1]) )\n",
        "    print(grads_vars[0])\n",
        "    print(grads_vars[1])\n",
        "  return zip(grads_vars[0],grads_vars[1])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5rq5QoGMslz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_gradients(model,input,label):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss= compute_loss(model,input,label)\n",
        "    print(model.trainable_variables)\n",
        "  \n",
        "  grads_vars=[]  \n",
        "#   for grad,var in tape.gradient(loss,model.trainable_variables):\n",
        "#     grads_vars.append([tf.reshape(grad,[-1]) , tf.reshape(var,[-1])])\n",
        "#   return grads_vars\n",
        "  return tape.gradient(loss,model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Goblt98Wjmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_meta_gradients(loss, tvars):\n",
        "#   with tf.GradientTape() as tape:\n",
        "#     return tape.gradient(loss, tvars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9TQwL4QTz61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_meta_model_trainable_parameters(cell_list):\n",
        "  var = cell_list.trainable_variables\n",
        "  '''\n",
        "  for cell in cell_list :\n",
        "     var += cell.trainable_variables\n",
        "  '''    \n",
        "  #print(var)\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzT_edDKwpac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(model,input,label):\n",
        "  return tf.reduce_sum(model(input) - label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FpUYeWovlHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def update_model_params(g_new_vars):\n",
        "  for count,grad_var in enumerate(g_new_vars):\n",
        "    grad=grad_var[0]\n",
        "    var= grad_var[1]\n",
        "    var = var-learning_rate*grad \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qP_yXsXYaUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## trying out stacked LSTM cells\n",
        "\n",
        "n_samplings=10\n",
        "n_unroll=5\n",
        "n_dimension=1\n",
        "n_hidden=5\n",
        "n_layers=2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YedqrfJRMHTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# @tf.function\n",
        "def trainOptimizer(n_samplings,n_unroll,n_dimension,n_hidden,n_layers,batch_size):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    n_samplings: number of random function samplings. L will be averaged over T.\n",
        "    n_unroll: the trained optimizers were unrolled for 20 steps.\n",
        "    n_dimension: the dimension of input data space.\n",
        "    n_hidden: This will be used when we actually use this rnn to generate a search direction for the next time step.\n",
        "    n_layers: n_layer LSTM architecture.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    loss = 0\n",
        "    inp_dim = [batch_size,n_dimension]\n",
        "    out_dim = n_dimension\n",
        "    \n",
        "    model= build_model(out_dim)\n",
        "    model.build(inp_dim)\n",
        "    n_trainable_vars = get_num_parameters(model.trainable_variables)\n",
        "    print(\"n_trainable_vars= \",n_trainable_vars)\n",
        "    \n",
        "    for t in range(n_samplings):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "          cell_list = build_meta_model(n_hidden,n_layers,n_trainable_vars)\n",
        "          # random sampling of one instance of the quadratic function\n",
        "          batch_size = 1\n",
        "  #         state_list = [cell_list[i].zero_state(batch_size, tf.float32) for i in range(n_dimension)] \n",
        "          sum_f = 0\n",
        "          g_new_list = []\n",
        "          model= build_model(n_dimension)\n",
        "          y = tf.random.normal([n_dimension, 1])\n",
        "          x = tf.random.normal([n_dimension, 1])\n",
        "          grad_f = get_gradients(model,x,y)\n",
        "          grad_f= reshape_params(zip(grad_f,model.trainable_variables))\n",
        "\n",
        "          state_list = [cell_list[i].get_initial_state(batch_size=batch_size, dtype=tf.float32) for i in range(n_trainable_vars)] \n",
        "          new_grad_f=[]\n",
        "          new_grad_var=[]\n",
        "          loss = 0\n",
        "          for _ in range(n_unroll):\n",
        "            for i,grad_var in enumerate(grad_f):\n",
        "                cell = cell_list[i]; state = state_list[i]\n",
        "                grad_h_t = grad_var[0]        ## verify for the models.\n",
        "                #print(\"grad_h_t,\\n\",grad_h_t.shape,\"state\\n\",state)\n",
        "                grad_h_t =  tf.expand_dims(tf.expand_dims(grad_h_t,axis=0),axis=1)\n",
        "                #print(\"grad_h_t,\\n\",grad_h_t.shape,\"state\\n\",state)\n",
        "\n",
        "\n",
        "                #Inputs to different lstm are of different shapes so same Lstm model cannot be used\n",
        "                cell_output, state = cell(grad_h_t , state) # g_new should be a scalar b/c grad_h_t is a scalar\n",
        "                #print(cell_output, state )\n",
        "\n",
        "    #                 softmax_w = tf.Variable(tf.zeros([n_hidden,1]),name=\"softmax_w\")\n",
        "    #                 softmax_b = tf.Variable(tf.zeros([1]),name=\"softmax_b\")\n",
        "                g_new_i = cell_output\n",
        "#                 tape.watch(grad_var[1])\n",
        "                new_grad_f.append(g_new_i)\n",
        "                new_grad_var.append(grad_var[1])\n",
        "                state_list[i] = state # for the next t\n",
        "\n",
        "            #new_grad_var += new_grad_f\n",
        "            \n",
        "#             update_model_params(zip(new_grad_f,new_grad_var),model)\n",
        "            for count,grad in enumerate(new_grad_f):\n",
        "              #grad=grad_var[0]\n",
        "              #var= grad_var[1]\n",
        "              #print(\"This is Truth\",new_grad_var[0])\n",
        "              new_grad_var[count] += grad \n",
        "            \n",
        "            count=0\n",
        "            \n",
        "#             theta = tf.add(theta, g_new)\n",
        "#             for layer in model.layers:\n",
        "#               print(\"Old\",layer.get_weights())\n",
        "      \n",
        "            weights_update=[]   \n",
        "            last_used = 0\n",
        "            for i in range(len(model.layers)):\n",
        "                # check to make sure only conv and fully connected layers are assigned weights.\n",
        "                if 'conv' in model.layers[i].name or 'out' in model.layers[i].name or 'dense' in model.layers[i].name:\n",
        "                    weights_shape = model.layers[i].kernel.shape\n",
        "                    no_of_weights = tf.reduce_prod(weights_shape)\n",
        "                    new_weights = tf.reshape( tf.convert_to_tensor( new_grad_var[count:no_of_weights ] ), weights_shape)\n",
        "                    model.layers[i].kernel = new_weights\n",
        "                    last_used += no_of_weights\n",
        "\n",
        "                if model.layers[i].use_bias:\n",
        "                    weights_shape = model.layers[i].bias.shape\n",
        "                    no_of_weights = tf.reduce_prod(weights_shape)\n",
        "                    new_weights = tf.reshape(tf.convert_to_tensor( new_grad_var[count:count+no_of_weights ] ), weights_shape)\n",
        "                    model.layers[i].bias = new_weights\n",
        "                    last_used += no_of_weights\n",
        "                  \n",
        "#               for weights in layer.trainable_variables:\n",
        "#                 print(\"++++++++++++++++\\n\",weights)\n",
        "\n",
        "#                 weights_update.append( tf.reshape(tf.convert_to_tensor( new_grad_var[count:count+np.product(weights.shape) ] ) ,weights.shape ) ) \n",
        "#                 weights = tf.reshape(tf.convert_to_tensor( new_grad_var[count:count+np.product(weights.shape) ] ) ,weights.shape ) \n",
        "#                 count+=np.product(weights.shape)\n",
        "              \n",
        "#               layer.set_weights(  np.array(weights_update) ) \n",
        "            \n",
        "#               print(\"Old\",layer.get_weights())\n",
        "              \n",
        "            for layer in model.layers:\n",
        "#               print(\"New\",layer.get_weights())\n",
        "                pass\n",
        "            loss += compute_loss(model,x,y)\n",
        "            #loss += tf.reduce_sum(model(x) - y)\n",
        "          print(loss)\n",
        "          loss = loss / n_unroll    \n",
        "            \n",
        "      # update parameter \n",
        "  #         g_new = tf.reshape(tf.squeeze(tf.stack(g_new_list)), [n_dimension, 1]) # should be a [n_dimension, 1] tensor\n",
        "          #print(new_grad_f)\n",
        "\n",
        "          #sum_f = sum_f + f_at_theta_t \n",
        "\n",
        "          #loss += sum_\n",
        "#           print( \"watched vars\", tape.watched_variables() )\n",
        "\n",
        "          #tvars = get_meta_model_trainable_parameters(cell_list[0])\n",
        "          tvars = cell_list[0].trainable_variables\n",
        "          #tvars = tf.trainable_variables() # should be just the variable inside the RNN\n",
        "#           grads = tape.gradient(new_grad_var, tvars)\n",
        "          grads = tape.gradient(loss, tvars)\n",
        "          print(\"----------------\\n\",grads,\"----------------\\n\")\n",
        "#           lr = 0.001 # Technically I need to do random search to decide this\n",
        "          optimizer = tf.keras.optimizers.Adam()\n",
        "          train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn_-VJRLMVWK",
        "colab_type": "code",
        "outputId": "663ca176-09e6-4d53-a4cb-63da396ecf50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "source": [
        "n_samplings=1\n",
        "n_unroll=1\n",
        "n_dimension=3\n",
        "n_hidden=5\n",
        "n_layers=2\n",
        "batch_size=1\n",
        "learning_rate=0.001  ## meta_learning_rate     \n",
        "\n",
        "trainOptimizer(n_samplings,n_unroll,n_dimension,n_hidden,n_layers,batch_size)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "20\n",
            "35\n",
            "38\n",
            "n_trainable_vars=  38\n",
            "[<tf.Variable 'sequential_23/dense_45/kernel:0' shape=(1, 5) dtype=float32, numpy=\n",
            "array([[-0.4728725 ,  0.9244702 ,  0.3920691 , -0.7081981 ,  0.68729067]],\n",
            "      dtype=float32)>, <tf.Variable 'sequential_23/dense_45/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'sequential_23/dense_46/kernel:0' shape=(5, 3) dtype=float32, numpy=\n",
            "array([[-0.15573496,  0.5542514 ,  0.6054701 ],\n",
            "       [ 0.40539628, -0.05237812, -0.82890296],\n",
            "       [-0.53846896,  0.00232846,  0.29455036],\n",
            "       [ 0.67539793,  0.45827943, -0.18992335],\n",
            "       [ 0.4051667 , -0.76457375,  0.5170613 ]], dtype=float32)>, <tf.Variable 'sequential_23/dense_46/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n",
            "<zip object at 0x7f13d8cd4e48>\n",
            "[<tf.Tensor: id=26788, shape=(), dtype=float32, numpy=2.094474>, <tf.Tensor: id=26789, shape=(), dtype=float32, numpy=-0.9927707>, <tf.Tensor: id=26790, shape=(), dtype=float32, numpy=-0.50399506>, <tf.Tensor: id=26791, shape=(), dtype=float32, numpy=1.9688196>, <tf.Tensor: id=26792, shape=(), dtype=float32, numpy=0.32889152>]\n",
            "[<tf.Tensor: id=26796, shape=(), dtype=float32, numpy=-0.4728725>, <tf.Tensor: id=26797, shape=(), dtype=float32, numpy=0.9244702>, <tf.Tensor: id=26798, shape=(), dtype=float32, numpy=0.3920691>, <tf.Tensor: id=26799, shape=(), dtype=float32, numpy=-0.7081981>, <tf.Tensor: id=26800, shape=(), dtype=float32, numpy=0.68729067>]\n",
            "[<tf.Tensor: id=26788, shape=(), dtype=float32, numpy=2.094474>, <tf.Tensor: id=26789, shape=(), dtype=float32, numpy=-0.9927707>, <tf.Tensor: id=26790, shape=(), dtype=float32, numpy=-0.50399506>, <tf.Tensor: id=26791, shape=(), dtype=float32, numpy=1.9688196>, <tf.Tensor: id=26792, shape=(), dtype=float32, numpy=0.32889152>, <tf.Tensor: id=26803, shape=(), dtype=float32, numpy=3.0119598>, <tf.Tensor: id=26804, shape=(), dtype=float32, numpy=-1.4276544>, <tf.Tensor: id=26805, shape=(), dtype=float32, numpy=-0.7247704>, <tf.Tensor: id=26806, shape=(), dtype=float32, numpy=2.831262>, <tf.Tensor: id=26807, shape=(), dtype=float32, numpy=0.47296268>]\n",
            "[<tf.Tensor: id=26796, shape=(), dtype=float32, numpy=-0.4728725>, <tf.Tensor: id=26797, shape=(), dtype=float32, numpy=0.9244702>, <tf.Tensor: id=26798, shape=(), dtype=float32, numpy=0.3920691>, <tf.Tensor: id=26799, shape=(), dtype=float32, numpy=-0.7081981>, <tf.Tensor: id=26800, shape=(), dtype=float32, numpy=0.68729067>, <tf.Tensor: id=26811, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26812, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26813, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26814, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26815, shape=(), dtype=float32, numpy=0.0>]\n",
            "[<tf.Tensor: id=26788, shape=(), dtype=float32, numpy=2.094474>, <tf.Tensor: id=26789, shape=(), dtype=float32, numpy=-0.9927707>, <tf.Tensor: id=26790, shape=(), dtype=float32, numpy=-0.50399506>, <tf.Tensor: id=26791, shape=(), dtype=float32, numpy=1.9688196>, <tf.Tensor: id=26792, shape=(), dtype=float32, numpy=0.32889152>, <tf.Tensor: id=26803, shape=(), dtype=float32, numpy=3.0119598>, <tf.Tensor: id=26804, shape=(), dtype=float32, numpy=-1.4276544>, <tf.Tensor: id=26805, shape=(), dtype=float32, numpy=-0.7247704>, <tf.Tensor: id=26806, shape=(), dtype=float32, numpy=2.831262>, <tf.Tensor: id=26807, shape=(), dtype=float32, numpy=0.47296268>, <tf.Tensor: id=26818, shape=(), dtype=float32, numpy=-0.98648643>, <tf.Tensor: id=26819, shape=(), dtype=float32, numpy=-0.98648643>, <tf.Tensor: id=26820, shape=(), dtype=float32, numpy=-0.98648643>, <tf.Tensor: id=26821, shape=(), dtype=float32, numpy=1.9285903>, <tf.Tensor: id=26822, shape=(), dtype=float32, numpy=1.9285903>, <tf.Tensor: id=26823, shape=(), dtype=float32, numpy=1.9285903>, <tf.Tensor: id=26824, shape=(), dtype=float32, numpy=0.8179179>, <tf.Tensor: id=26825, shape=(), dtype=float32, numpy=0.8179179>, <tf.Tensor: id=26826, shape=(), dtype=float32, numpy=0.8179179>, <tf.Tensor: id=26827, shape=(), dtype=float32, numpy=-1.4774127>, <tf.Tensor: id=26828, shape=(), dtype=float32, numpy=-1.4774127>, <tf.Tensor: id=26829, shape=(), dtype=float32, numpy=-1.4774127>, <tf.Tensor: id=26830, shape=(), dtype=float32, numpy=1.4337965>, <tf.Tensor: id=26831, shape=(), dtype=float32, numpy=1.4337965>, <tf.Tensor: id=26832, shape=(), dtype=float32, numpy=1.4337965>]\n",
            "[<tf.Tensor: id=26796, shape=(), dtype=float32, numpy=-0.4728725>, <tf.Tensor: id=26797, shape=(), dtype=float32, numpy=0.9244702>, <tf.Tensor: id=26798, shape=(), dtype=float32, numpy=0.3920691>, <tf.Tensor: id=26799, shape=(), dtype=float32, numpy=-0.7081981>, <tf.Tensor: id=26800, shape=(), dtype=float32, numpy=0.68729067>, <tf.Tensor: id=26811, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26812, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26813, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26814, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26815, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26836, shape=(), dtype=float32, numpy=-0.15573496>, <tf.Tensor: id=26837, shape=(), dtype=float32, numpy=0.5542514>, <tf.Tensor: id=26838, shape=(), dtype=float32, numpy=0.6054701>, <tf.Tensor: id=26839, shape=(), dtype=float32, numpy=0.40539628>, <tf.Tensor: id=26840, shape=(), dtype=float32, numpy=-0.052378118>, <tf.Tensor: id=26841, shape=(), dtype=float32, numpy=-0.82890296>, <tf.Tensor: id=26842, shape=(), dtype=float32, numpy=-0.53846896>, <tf.Tensor: id=26843, shape=(), dtype=float32, numpy=0.0023284554>, <tf.Tensor: id=26844, shape=(), dtype=float32, numpy=0.29455036>, <tf.Tensor: id=26845, shape=(), dtype=float32, numpy=0.67539793>, <tf.Tensor: id=26846, shape=(), dtype=float32, numpy=0.45827943>, <tf.Tensor: id=26847, shape=(), dtype=float32, numpy=-0.18992335>, <tf.Tensor: id=26848, shape=(), dtype=float32, numpy=0.4051667>, <tf.Tensor: id=26849, shape=(), dtype=float32, numpy=-0.76457375>, <tf.Tensor: id=26850, shape=(), dtype=float32, numpy=0.5170613>]\n",
            "[<tf.Tensor: id=26788, shape=(), dtype=float32, numpy=2.094474>, <tf.Tensor: id=26789, shape=(), dtype=float32, numpy=-0.9927707>, <tf.Tensor: id=26790, shape=(), dtype=float32, numpy=-0.50399506>, <tf.Tensor: id=26791, shape=(), dtype=float32, numpy=1.9688196>, <tf.Tensor: id=26792, shape=(), dtype=float32, numpy=0.32889152>, <tf.Tensor: id=26803, shape=(), dtype=float32, numpy=3.0119598>, <tf.Tensor: id=26804, shape=(), dtype=float32, numpy=-1.4276544>, <tf.Tensor: id=26805, shape=(), dtype=float32, numpy=-0.7247704>, <tf.Tensor: id=26806, shape=(), dtype=float32, numpy=2.831262>, <tf.Tensor: id=26807, shape=(), dtype=float32, numpy=0.47296268>, <tf.Tensor: id=26818, shape=(), dtype=float32, numpy=-0.98648643>, <tf.Tensor: id=26819, shape=(), dtype=float32, numpy=-0.98648643>, <tf.Tensor: id=26820, shape=(), dtype=float32, numpy=-0.98648643>, <tf.Tensor: id=26821, shape=(), dtype=float32, numpy=1.9285903>, <tf.Tensor: id=26822, shape=(), dtype=float32, numpy=1.9285903>, <tf.Tensor: id=26823, shape=(), dtype=float32, numpy=1.9285903>, <tf.Tensor: id=26824, shape=(), dtype=float32, numpy=0.8179179>, <tf.Tensor: id=26825, shape=(), dtype=float32, numpy=0.8179179>, <tf.Tensor: id=26826, shape=(), dtype=float32, numpy=0.8179179>, <tf.Tensor: id=26827, shape=(), dtype=float32, numpy=-1.4774127>, <tf.Tensor: id=26828, shape=(), dtype=float32, numpy=-1.4774127>, <tf.Tensor: id=26829, shape=(), dtype=float32, numpy=-1.4774127>, <tf.Tensor: id=26830, shape=(), dtype=float32, numpy=1.4337965>, <tf.Tensor: id=26831, shape=(), dtype=float32, numpy=1.4337965>, <tf.Tensor: id=26832, shape=(), dtype=float32, numpy=1.4337965>, <tf.Tensor: id=26853, shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: id=26854, shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: id=26855, shape=(), dtype=float32, numpy=3.0>]\n",
            "[<tf.Tensor: id=26796, shape=(), dtype=float32, numpy=-0.4728725>, <tf.Tensor: id=26797, shape=(), dtype=float32, numpy=0.9244702>, <tf.Tensor: id=26798, shape=(), dtype=float32, numpy=0.3920691>, <tf.Tensor: id=26799, shape=(), dtype=float32, numpy=-0.7081981>, <tf.Tensor: id=26800, shape=(), dtype=float32, numpy=0.68729067>, <tf.Tensor: id=26811, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26812, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26813, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26814, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26815, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26836, shape=(), dtype=float32, numpy=-0.15573496>, <tf.Tensor: id=26837, shape=(), dtype=float32, numpy=0.5542514>, <tf.Tensor: id=26838, shape=(), dtype=float32, numpy=0.6054701>, <tf.Tensor: id=26839, shape=(), dtype=float32, numpy=0.40539628>, <tf.Tensor: id=26840, shape=(), dtype=float32, numpy=-0.052378118>, <tf.Tensor: id=26841, shape=(), dtype=float32, numpy=-0.82890296>, <tf.Tensor: id=26842, shape=(), dtype=float32, numpy=-0.53846896>, <tf.Tensor: id=26843, shape=(), dtype=float32, numpy=0.0023284554>, <tf.Tensor: id=26844, shape=(), dtype=float32, numpy=0.29455036>, <tf.Tensor: id=26845, shape=(), dtype=float32, numpy=0.67539793>, <tf.Tensor: id=26846, shape=(), dtype=float32, numpy=0.45827943>, <tf.Tensor: id=26847, shape=(), dtype=float32, numpy=-0.18992335>, <tf.Tensor: id=26848, shape=(), dtype=float32, numpy=0.4051667>, <tf.Tensor: id=26849, shape=(), dtype=float32, numpy=-0.76457375>, <tf.Tensor: id=26850, shape=(), dtype=float32, numpy=0.5170613>, <tf.Tensor: id=26859, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26860, shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: id=26861, shape=(), dtype=float32, numpy=0.0>]\n",
            "tf.Tensor(5.540533, shape=(), dtype=float32)\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "----------------\n",
            " [<tf.Tensor: id=29629, shape=(1, 20), dtype=float32, numpy=\n",
            "array([[ 0.16734979,  0.6646755 ,  0.6756332 , -0.42731595,  0.01877171,\n",
            "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
            "        -0.49524873, -0.71618026,  1.4358666 , -0.2589406 , -0.02444482,\n",
            "         0.13283345,  0.43437576,  0.57105947, -0.17367667,  0.01441911]],\n",
            "      dtype=float32)>, <tf.Tensor: id=29630, shape=(5, 20), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: id=29631, shape=(20,), dtype=float32, numpy=\n",
            "array([ 0.07273638,  0.2839745 ,  0.2819443 , -0.1883138 ,  0.00747301,\n",
            "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
            "       -0.34604645, -0.6957877 ,  1.1178542 , -0.39048994, -0.0483501 ,\n",
            "        0.05682462,  0.17534044,  0.23349917, -0.05508761,  0.00439381],\n",
            "      dtype=float32)>, <tf.Tensor: id=29632, shape=(5, 4), dtype=float32, numpy=\n",
            "array([[-0.22856931,  0.        , -1.488468  , -0.25982824],\n",
            "       [-0.19676901,  0.        , -1.2808616 , -0.22301874],\n",
            "       [ 0.13110264,  0.        ,  0.83678335,  0.14865704],\n",
            "       [ 0.10858729,  0.        ,  0.70349336,  0.12221986],\n",
            "       [-0.10555552,  0.        , -0.66561455, -0.11927787]],\n",
            "      dtype=float32)>, <tf.Tensor: id=29633, shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: id=29634, shape=(4,), dtype=float32, numpy=array([0.7485722, 0.       , 7.649987 , 0.8475361], dtype=float32)>] ----------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxC1cIMSdGkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir /tmp/tutorial/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1T8d4nbdE61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /tmp/tutorial/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHcl-J5GPlEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}